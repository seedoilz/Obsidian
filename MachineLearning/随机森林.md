---
aliases: [random forest]
date created: 四月 19日 2023, 2:23:38 下午
date modified: 三月 5日 2024, 4:07:12 下午
title: 随机森林
tags: [code/machine-learning]
---

## 原理
有 N 颗[[决策树]]，每一颗决策树只专注于处理某一部分数据（也只关注某一部分特征），每一颗决策树在处理某些特征时候的表现会比较优良，从而整合所有决策树的结果，最后产生一个更好的结果。

### 样本随机
随机选取部分样本进行训练🏋️，能够使得决策边界更加平滑和准确。

### 特征随机
随机选取部分特征进行训练🏋️，引入非线形，让决策边界更加合理。

## 实例
### 引入
```python
import numpy as np  
import scipy  
import pandas as pd  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV  
from sklearn.metrics import roc_curve, auc, roc_auc_score  
import matplotlib.pyplot as plt
```

### 数据处理
与 XGBoost 类似，都是将 X 和 Y 进行分离，X 为输入，Y 为实际要输出的内容。
```python
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=seed)
```

### 实际训练
```python
rfc = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42) #实例化

rfc = rfc.fit(xtrain,ytrain) #用训练集数据训练模型  
result = rfc.score(xtest,ytest) #导入测试集，rfc的接口score计算的是模型准确率accuracy
```

### 重要性程度可视化
```python
importances = rfc.feature_importances_   #获取随机森林分类器中每个特征的重要性得分，存储在`importances`数组中。
std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0) #计算每个特征的重要性得分的标准差，这个标准差可以反映特征重要性得分的稳定性。这里使用了随机森林中每棵树中特征重要性得分的标准差的平均值作为特征重要性得分的标准差。
indices = np.argsort(importances)[::-1] #计算每个特征的重要性得分的标准差，这个标准差可以反映特征重要性得分的稳定性。这里使用了随机森林中每棵树中特征重要性得分的标准差的平均值作为特征重要性得分的标准差。
print("Feature ranking:")  
for f in range(min(20,xtrain.shape[1])):  
	print("%2d) %-*s %f" % (f + 1, 30, xtrain.columns[indices[f]], importances[indices[f]]))# Plot the feature importances of the forest  
plt.figure()  
plt.title("Feature importances")  
plt.bar(range(xtrain.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")  
plt.xticks(range(xtrain.shape[1]), indices)  
plt.xlim([-1, xtrain.shape[1]])  
plt.show()
```