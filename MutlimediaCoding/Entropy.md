---
aliases: 
tags:
  - code/coding
title: Entropy
date created: 2025-02-06 20:02:00
date modified: 2025-02-06 20:02:62
---
From information theory, the entropy of a discrete random variable X with possible values ${x_1, â€¦, x_n}$ and probability mass function $P(X)$ is written as:
$$
H(X) = - \sum_i P(x_i) \log P(x_i)
$$Entropy is lower bound of **Average bit rate**